{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning using MNIST data\n",
    "To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n",
    "\n",
    "Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_transfer_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "def view_layer(model):\n",
    "    print('='*100)\n",
    "    trainable_params = non_trainable_params = 0\n",
    "    for i, layer in enumerate(model.layers, start=1):\n",
    "        if layer.weights:\n",
    "            n_params = K.count_params(layer.weights[0])+K.count_params(layer.weights[1])\n",
    "            if layer.trainable:\n",
    "                trainable_params += n_params\n",
    "            else:\n",
    "                non_trainable_params += n_params\n",
    "            print('Layer {:02d}: {:15} - Trainable: {:5} - #Params: {:9,d} - Output Shape: {}'.format( \\\n",
    "                i, layer.name[:15], str(layer.trainable), n_params, layer.output_shape))\n",
    "        else:\n",
    "            print('Layer {:02d}: {:15} - Trainable: {:5} - {} - Output Shape: {}'.format( \\\n",
    "                i, layer.name[:15], str(layer.trainable), 18*' ', layer.output_shape))  \n",
    "    print('='*100)\n",
    "    print('Total params: {:,d}'.format(trainable_params + non_trainable_params))\n",
    "    print('Trainable params: {:,d}'.format(trainable_params))\n",
    "    print('Non-trainable params: {:,d}'.format(non_trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to help some of the timing functions\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "batch_size = 128\n",
    "num_classes = 5\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some more parameters\n",
    "# MNIST 28x28\n",
    "\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This just handles some variability in how the input data is loaded\n",
    "# if backend requied inoput shape channels first\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To simplify things, write a function to include all the training steps\n",
    "## As input, function takes a model, training set, test set, and the number of classes\n",
    "## Inside the model object will be the state about which layers we are freezing and which we are training\n",
    "\n",
    "\n",
    "# must convert dat ato float32 !! It's faster than float64\n",
    "# if no problem in precition of 32\n",
    "\n",
    "def train_model(model, train, test, num_classes, epochs):\n",
    "    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n",
    "    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n",
    "    # retype to float32\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    # normalization\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(train[1], num_classes)\n",
    "    y_test = keras.utils.to_categorical(test[1], num_classes)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='sgd',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    t = now()\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Training time: %s' % (now() - t))\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create two datasets: one with digits below 5 and one with 5 and above\n",
    "x_train_lt5 = x_train[y_train < 5]\n",
    "y_train_lt5 = y_train[y_train < 5]\n",
    "x_test_lt5 = x_test[y_test < 5]\n",
    "y_test_lt5 = y_test[y_test < 5]\n",
    "\n",
    "x_train_gte5 = x_train[y_train >= 5]\n",
    "y_train_gte5 = y_train[y_train >= 5] - 5\n",
    "x_test_gte5 = x_test[y_test >= 5]\n",
    "y_test_gte5 = y_test[y_test >= 5] - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separate feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"feature\" layers.  These are the early layers that we expect will \"transfer\"\n",
    "# to a new problem.  We will freeze these layers during the fine-tuning process\n",
    "\n",
    "feature_layers = [\n",
    "    Conv2D(16, kernel_size=(3, 3),\n",
    "           padding='valid',\n",
    "           input_shape=input_shape, activation='relu'),\n",
    "    Conv2D(16, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separate classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"classification\" layers.  These are the later layers that predict the specific classes from the features\n",
    "# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 13:36:59.384119: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Max\n",
      "2024-01-10 13:36:59.384138: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 36.00 GB\n",
      "2024-01-10 13:36:59.384143: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 13.50 GB\n",
      "2024-01-10 13:36:59.384189: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-10 13:36:59.384216: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# We create our model by combining the two sets of layers as follows\n",
    "model = Sequential(feature_layers + classification_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 12, 12, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 16)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 298165 (1.14 MB)\n",
      "Trainable params: 298165 (1.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (29404, 28, 28, 1)\n",
      "29404 train samples\n",
      "4861 test samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 13:37:00.080231: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/230 [==============================] - 6s 15ms/step - loss: 1.8716 - accuracy: 0.3577 - val_loss: 0.7897 - val_accuracy: 0.6785\n",
      "Epoch 2/10\n",
      "230/230 [==============================] - 3s 12ms/step - loss: 1.7843 - accuracy: 0.3323 - val_loss: 1.6963 - val_accuracy: 0.2076\n",
      "Epoch 3/10\n",
      "230/230 [==============================] - 3s 13ms/step - loss: 1.7141 - accuracy: 0.2038 - val_loss: 1.6150 - val_accuracy: 0.1971\n",
      "Epoch 4/10\n",
      "230/230 [==============================] - 3s 12ms/step - loss: 1.6811 - accuracy: 0.2034 - val_loss: 1.6602 - val_accuracy: 0.1835\n",
      "Epoch 5/10\n",
      "230/230 [==============================] - 3s 11ms/step - loss: 1.6774 - accuracy: 0.2033 - val_loss: 1.6592 - val_accuracy: 0.1835\n",
      "Epoch 6/10\n",
      "230/230 [==============================] - 3s 12ms/step - loss: 1.6675 - accuracy: 0.2061 - val_loss: 1.6756 - val_accuracy: 0.1835\n",
      "Epoch 7/10\n",
      "230/230 [==============================] - 3s 11ms/step - loss: 1.6740 - accuracy: 0.2027 - val_loss: 1.6194 - val_accuracy: 0.2004\n",
      "Epoch 8/10\n",
      "230/230 [==============================] - 3s 12ms/step - loss: 1.6688 - accuracy: 0.2003 - val_loss: 1.6360 - val_accuracy: 0.1835\n",
      "Epoch 9/10\n",
      "230/230 [==============================] - 3s 12ms/step - loss: 1.6630 - accuracy: 0.2029 - val_loss: 1.6547 - val_accuracy: 0.2004\n",
      "Epoch 10/10\n",
      "230/230 [==============================] - 3s 12ms/step - loss: 1.6651 - accuracy: 0.2054 - val_loss: 1.6655 - val_accuracy: 0.1835\n",
      "Training time: 0:00:30.147424\n",
      "Test score: 1.6655348539352417\n",
      "Test accuracy: 0.18350133299827576\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train our model on the digits 5,6,7,8,9\n",
    "\n",
    "train_model(model,\n",
    "            (x_train_gte5, y_train_gte5),\n",
    "            (x_test_gte5, y_test_gte5), num_classes, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing Layers\n",
    "Keras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n",
    "\n",
    "Note also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking Inside Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Code for Viewing Layers](https://docs.google.com/document/d/1iACjBPWTWvXTSecBNHx1xEtODcHSpnU6HFY82b4b6Ys)\n",
    "\n",
    "#### Copy and Paste the code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze CNN Layers (Low-level or Early Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer 01: conv2d          - Trainable: True  - #Params:       160 - Output Shape: (None, 26, 26, 16)\n",
      "Layer 02: conv2d_1        - Trainable: True  - #Params:     2,320 - Output Shape: (None, 24, 24, 16)\n",
      "Layer 03: max_pooling2d   - Trainable: True  -                    - Output Shape: (None, 12, 12, 16)\n",
      "Layer 04: dropout         - Trainable: True  -                    - Output Shape: (None, 12, 12, 16)\n",
      "Layer 05: flatten         - Trainable: True  -                    - Output Shape: (None, 2304)\n",
      "Layer 06: dense           - Trainable: True  - #Params:   295,040 - Output Shape: (None, 128)\n",
      "Layer 07: dropout_1       - Trainable: True  -                    - Output Shape: (None, 128)\n",
      "Layer 08: dense_1         - Trainable: True  - #Params:       645 - Output Shape: (None, 5)\n",
      "====================================================================================================\n",
      "Total params: 298,165\n",
      "Trainable params: 298,165\n",
      "Non-trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "# model.layers[0].trainable\n",
    "view_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze only the \n",
    "for l in feature_layers:\n",
    "    l.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer 01: conv2d          - Trainable: False - #Params:       160 - Output Shape: (None, 26, 26, 16)\n",
      "Layer 02: conv2d_1        - Trainable: False - #Params:     2,320 - Output Shape: (None, 24, 24, 16)\n",
      "Layer 03: max_pooling2d   - Trainable: False -                    - Output Shape: (None, 12, 12, 16)\n",
      "Layer 04: dropout         - Trainable: False -                    - Output Shape: (None, 12, 12, 16)\n",
      "Layer 05: flatten         - Trainable: False -                    - Output Shape: (None, 2304)\n",
      "Layer 06: dense           - Trainable: True  - #Params:   295,040 - Output Shape: (None, 128)\n",
      "Layer 07: dropout_1       - Trainable: True  -                    - Output Shape: (None, 128)\n",
      "Layer 08: dense_1         - Trainable: True  - #Params:       645 - Output Shape: (None, 5)\n",
      "====================================================================================================\n",
      "Total params: 298,165\n",
      "Trainable params: 295,685\n",
      "Non-trainable params: 2,480\n"
     ]
    }
   ],
   "source": [
    "view_layer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 12, 12, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 12, 12, 16)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               295040    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 298165 (1.14 MB)\n",
      "Trainable params: 295685 (1.13 MB)\n",
      "Non-trainable params: 2480 (9.69 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (30596, 28, 28, 1)\n",
      "30596 train samples\n",
      "5139 test samples\n",
      "Epoch 1/2\n",
      "240/240 [==============================] - 3s 12ms/step - loss: 1.6669 - accuracy: 0.2039 - val_loss: 3.1623 - val_accuracy: 0.1911\n",
      "Epoch 2/2\n",
      "240/240 [==============================] - 2s 10ms/step - loss: 1.6722 - accuracy: 0.2025 - val_loss: 2.8457 - val_accuracy: 0.1911\n",
      "Training time: 0:00:05.512679\n",
      "Test score: 2.8456904888153076\n",
      "Test accuracy: 0.1910877674818039\n"
     ]
    }
   ],
   "source": [
    "train_model(model,\n",
    "            (x_train_lt5, y_train_lt5),\n",
    "            (x_test_lt5, y_test_lt5), num_classes, epochs//4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n",
    "\n",
    "Also, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "### To do:\n",
    "- Now write code to reverse this training process.  That is, you will train on the digits 0-4, and then finetune only the last layers on the digits 5-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
